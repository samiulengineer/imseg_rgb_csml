{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import config\n",
    "import pathlib\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import *\n",
    "import earthpy.plot as ep\n",
    "import earthpy.spatial as es\n",
    "from dataset import read_img\n",
    "from matplotlib import pyplot as plt\n",
    "import subprocess\n",
    "import pyperclip\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(config.train_dir)\n",
    "test_df =  pd.read_csv(config.test_dir)\n",
    "valid_df = pd.read_csv(config.valid_dir)\n",
    "p_train_json = config.p_train_dir\n",
    "p_test_json = config.p_test_dir\n",
    "p_valid_json = config.p_valid_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images = 1640\n",
      "Total number of test images = 205\n",
      "Total number of validation images = 205\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of training images = {len(train_df)}\")\n",
    "print(f\"Total number of test images = {len(test_df)}\")\n",
    "print(f\"Total number of validation images = {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance_check(patchify, data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        checking class percentage in full dataset\n",
    "    Arguments:\n",
    "        patchify (bool): TRUE if want to check class balance for patchify experiments\n",
    "        data_dir (str): directory where data files are saved \n",
    "    Return:\n",
    "        class percentage\n",
    "    \"\"\"\n",
    "    if patchify:\n",
    "        with open(data_dir, \"r\") as j:\n",
    "            train_data = json.loads(j.read())\n",
    "        labels = train_data[\"masks\"]\n",
    "        patch_idx = train_data[\"patch_idx\"]\n",
    "\n",
    "    # commented out by manik (as patchify is false, it's CFR or CFR_CB, so it's deprecated)\n",
    "    else:\n",
    "        train_data = pd.read_csv(data_dir)\n",
    "        labels = train_data.masks.values\n",
    "        patch_idx = None\n",
    "\n",
    "    total = 0\n",
    "    class_name = {}\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        mask = cv2.imread(labels[i])\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        # mask[mask < 105] = 0\n",
    "        # mask[mask > 104] = 1\n",
    "        if patchify:\n",
    "            idx = patch_idx[i]\n",
    "            mask = mask[idx[0] : idx[1], idx[2] : idx[3]]\n",
    "\n",
    "        total_pix = mask.shape[0] * mask.shape[1]\n",
    "        total += total_pix\n",
    "\n",
    "        dic = {}\n",
    "        keys = np.unique(mask)\n",
    "        for i in keys:\n",
    "            dic[i] = np.count_nonzero(mask == i)\n",
    "\n",
    "        for key, value in dic.items():\n",
    "            if key in class_name.keys():\n",
    "                #problems\n",
    "                class_name[key] = value + class_name[key]\n",
    "            else:\n",
    "                class_name[key] = value\n",
    "\n",
    "    for key, val in class_name.items():\n",
    "        class_name[key] = (val / total) * 100\n",
    "\n",
    "    print(\"Class percentage:\")\n",
    "    for key, val in class_name.items():\n",
    "        print(\"class pixel: {} = {}\".format(key, val))\n",
    "    print(f\"unique value in the mask {class_name.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class percentage of traning data before patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class percentage:\n",
      "class pixel: 0 = 66.12067807136405\n",
      "class pixel: 170 = 1.5814504290876243\n",
      "class pixel: 255 = 4.919822719060524\n",
      "class pixel: 85 = 27.378048780487806\n",
      "unique value in the mask dict_keys([0, 170, 255, 85])\n",
      ".........................................................................................\n",
      "class percentage of traning data after patch\n",
      "Class percentage:\n",
      "class pixel: 0 = 72.1110937952811\n",
      "class pixel: 170 = 5.855161341608707\n",
      "class pixel: 255 = 22.03374486311019\n",
      "unique value in the mask dict_keys([0, 170, 255])\n"
     ]
    }
   ],
   "source": [
    "print(\"class percentage of traning data before patch\")\n",
    "class_balance_check(patchify=False, data_dir=config.train_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"class percentage of traning data after patch\")\n",
    "class_balance_check(patchify=True, data_dir=config.p_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_height_width(data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        check unique hight and width of images from dataset\n",
    "    Arguments:\n",
    "        data_dir (str): path to csv file\n",
    "    Return:\n",
    "        print all the unique height and width\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_dir)\n",
    "    # removing UU or UMM or UM\n",
    "    # data = data[data['feature_ids'].str.contains('uu_00') == False]\n",
    "    #problems\n",
    "    # data = data[data[\"feature_ids\"].str.contains(\"umm_00\") == False]\n",
    "    # data = data[data[\"feature_ids\"].str.contains(\"um_00\") == False]\n",
    "\n",
    "    print(\"Dataset:  \", data.shape)\n",
    "\n",
    "    input_img = data.feature_ids.values\n",
    "    input_mask = data.masks.values\n",
    "\n",
    "    input_img_shape = []\n",
    "    input_mask_shape = []\n",
    "\n",
    "    for i in range(len(input_img)):\n",
    "        img = cv2.imread(input_img[i])\n",
    "        mask = cv2.imread(input_mask[i])\n",
    "\n",
    "        if img.shape not in input_img_shape:\n",
    "            input_img_shape.append(img.shape)\n",
    "\n",
    "        if mask.shape not in input_mask_shape:\n",
    "            input_mask_shape.append(mask.shape)\n",
    "\n",
    "    print(\"Input image shapes: \", input_img_shape)\n",
    "    print(\"Input mask shapes: \", input_mask_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique height and width of training dataset\n",
      "Dataset:   (1640, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shapes:  [(240, 360, 3)]\n",
      "Input mask shapes:  [(240, 360, 3)]\n",
      ".........................................................................................\n",
      "Unique height and width of testing dataset\n",
      "Dataset:   (205, 2)\n",
      "Input image shapes:  [(240, 360, 3)]\n",
      "Input mask shapes:  [(240, 360, 3)]\n",
      ".........................................................................................\n",
      "Unique height and width of validation dataset\n",
      "Dataset:   (205, 2)\n",
      "Input image shapes:  [(240, 360, 3)]\n",
      "Input mask shapes:  [(240, 360, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique height and width of training dataset\")\n",
    "check_height_width(config.train_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"Unique height and width of testing dataset\")\n",
    "check_height_width(config.test_dir)\n",
    "print(\".........................................................................................\")\n",
    "\n",
    "print(\"Unique height and width of validation dataset\")\n",
    "check_height_width(config.valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_csv_from_path(csv_path=config.csv_logger_path):\n",
    "    csv_list = []\n",
    "    # Iterate through each subdirectory\n",
    "    for folder in csv_path.iterdir():\n",
    "        # Check if the entry is a directory\n",
    "        if folder.is_dir():\n",
    "            # Iterate through files in the subdirectory\n",
    "            for file in folder.iterdir():\n",
    "                # Check if the entry is a file\n",
    "                if file.is_file():\n",
    "                    csv_list.append(file)\n",
    "    # print(csv_list)\n",
    "    return csv_list\n",
    "                    \n",
    "\n",
    "def _plot_from_csv(csv_path, name, x_axis_name, y_axis_name, columns_to_plot=None):\n",
    "    pathlib.Path((config.root_dir /\"logs\" / \"plots\"/\"metrics_plots\")).mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    epochs = df['epoch']\n",
    "    if columns_to_plot is not None:\n",
    "        columns_to_plot = columns_to_plot\n",
    "    else:\n",
    "        columns_to_plot = df.columns.to_list()[1:]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for column in columns_to_plot:\n",
    "        plt.plot(epochs, df[column], label=column, linewidth=3.0,\n",
    "            marker=\"o\",\n",
    "            markersize=5)\n",
    "\n",
    "    plt.title(f\"{y_axis_name}_over_{x_axis_name}\")\n",
    "    plt.xlabel(x_axis_name)\n",
    "    plt.ylabel(y_axis_name)\n",
    "    plt.xticks(epochs.astype(int))\n",
    "    plt.legend()\n",
    "    plt.savefig(config.root_dir/\"logs\"/\"plots\"/\"metrics_plots\"/name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics_vs_epochs(csv_path, name, x_axis_name= \"Epochs\", y_axis_name=\"Metrics_score\",columns_to_plot=None):\n",
    "    _plot_from_csv(csv_path=csv_path, name=name,x_axis_name=x_axis_name, y_axis_name=y_axis_name, columns_to_plot=columns_to_plot)\n",
    "\n",
    "def plot_metric_vs_epochs_vs_models(metric_name=\"val_f1_score\"):\n",
    "    pathlib.Path((config.root_dir /\"logs\"/ \"plots\"/\"csv_for_plotting\")).mkdir(parents=True, exist_ok=True)\n",
    "    csv_list = return_csv_from_path()\n",
    "    result_df = pd.DataFrame()\n",
    "    for csv_path in csv_list:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        result_df[os.path.basename(csv_path)] = df[metric_name]\n",
    "    result_df.index.name = \"epoch\"\n",
    "    result_df.to_csv(os.path.join(config.root_dir/\"logs\"/\"plots\"/\"csv_for_plotting\"/f\"{metric_name}_vs_epoch.csv\"), encoding='utf-8',index=True, header=True)\n",
    "    _plot_from_csv(config.root_dir/\"logs\"/\"plots\"/\"csv_for_plotting\"/f\"{metric_name}_vs_epoch.csv\", x_axis_name= \"Epochs\", y_axis_name=metric_name, name=metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_vs_epochs(config.csv_logger_path/\"unet\"/\"unet_ex_training_ep_20.csv\",name='metrics')\n",
    "plot_metrics_vs_epochs(config.csv_logger_path/\"unet\"/\"unet_ex_training_ep_20.csv\",name='metrics',columns_to_plot=[\"f1_score\"])\n",
    "plot_metric_vs_epochs_vs_models()\n",
    "plot_metric_vs_epochs_vs_models(metric_name=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(data, name):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        save all images into single figure\n",
    "    Arguments:\n",
    "        data : data file holding images path\n",
    "        directory (str) : path to save images\n",
    "    Return:\n",
    "        save images figure into directory\n",
    "    \"\"\"\n",
    "\n",
    "    pathlib.Path((visualization_dir / \"display\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"train\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"test\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"valid\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        image = read_img(data.feature_ids.values[i])\n",
    "        mask = read_img(data.masks.values[i], label=True)\n",
    "        id = data.feature_ids.values[i].split(\"/\")[-1]\n",
    "        display_list = {\"image\": image, \"label\": mask}\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        title = list(display_list.keys())\n",
    "\n",
    "        for i in range(len(display_list)):\n",
    "            plt.subplot(1, len(display_list), i + 1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow((display_list[title[i]]), cmap=\"gray\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        prediction_name = \"img_id_{}\".format(id)  # create file name to save\n",
    "        plt.savefig(\n",
    "            os.path.join((visualization_dir / \"display\"/ name), prediction_name),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=800,\n",
    "        )\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying training images and masks\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying training images and masks\")\n",
    "display_all(data=train_df,name=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"displaying testing images and masks\")\n",
    "display_all(data=test_df, name = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"displaying validation images and masks\")\n",
    "display_all(data=valid_df, name= \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/hdd2/mdsamiul/project/dataset/rice_data_training/awc00_vv.tif', '/mnt/hdd2/mdsamiul/project/dataset/rice_data_training/awc00_vh.tif', '/mnt/hdd2/mdsamiul/project/dataset/rice_data_training/awc00_nasadem.tif']\n",
      "...............................\n",
      "-14.243971\n",
      "4.883937\n",
      "...............................\n",
      "...............................\n",
      "-22.869745\n",
      "6.5032916\n",
      "...............................\n",
      "...............................\n",
      "-14.243971\n",
      "4.883937\n",
      "...............................\n"
     ]
    }
   ],
   "source": [
    "eval_csv = pd.read_csv(\"/mnt/hdd2/mdsamiul/project/imseg_csml/data/csv/train.csv\")\n",
    "masks = eval_csv[\"feature_ids\"].to_list()\n",
    "ext = [\"_vv.tif\",\"_vh.tif\",\"_nasadem.tif\"]\n",
    "masks = masks[0]\n",
    "masks= [masks+ex for ex in ext]\n",
    "print(masks)\n",
    "for p in masks:\n",
    "    with rasterio.open(p) as im:\n",
    "        image = im.read(1)\n",
    "        print(\"...............................\")\n",
    "        # print(p)\n",
    "        # print(np.unique(image, return_counts=False))\n",
    "        print(np.mean(image))\n",
    "        print(np.std(image))\n",
    "        print(\"...............................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(datapath):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(datapath)\n",
    "    \n",
    "    for filename in files:\n",
    "        # Extract the file extension\n",
    "        _, ext = os.path.splitext(filename)\n",
    "        \n",
    "        # Check if the filename starts with DEM_ab.tif\n",
    "        if filename.startswith(\"DEM_\"):\n",
    "            new_filename = filename.replace(\"DEM_\", \"\").replace(\".tif\", \"_nasadem.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VV_ab.tif\n",
    "        elif filename.startswith(\"VV_\"):\n",
    "            new_filename = filename.replace(\"VV_\", \"\").replace(\".tif\", \"_vv.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VH_ab.tif\n",
    "        elif filename.startswith(\"VH_\"):\n",
    "            new_filename = filename.replace(\"VH_\", \"\").replace(\".tif\", \"_vh.tif\")\n",
    "        \n",
    "        # Check if the filename starts with GT_ab.tif\n",
    "        elif filename.startswith(\"GT_\"):\n",
    "            new_filename = filename.replace(\"GT_\", \"\")\n",
    "        \n",
    "        else:\n",
    "            # If none of the conditions are met, skip this file\n",
    "            raise ValueError(\"files_name_mismatch\")\n",
    "        \n",
    "        # Construct the new filepath\n",
    "        new_filepath = os.path.join(datapath, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(os.path.join(datapath, filename), new_filepath)\n",
    "        print(f\"Renamed {filename} to {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = config.dataset_dir\n",
    "rename_files(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal output saved to data_statistics.rtf\n"
     ]
    }
   ],
   "source": [
    "# Run the command in the terminal\n",
    "command = \"python visualization.py\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "# Get the terminal output\n",
    "terminal_output = result.stdout\n",
    "\n",
    "# Save the output to an RTF file\n",
    "rtf_filename = \"data_statistics.rtf\"\n",
    "with open(rtf_filename, \"w\") as rtf_file:\n",
    "    # rtf_file.write(\"{\\\\rtf1\\\\ansi\\n\")\n",
    "    rtf_file.write(terminal_output)\n",
    "    # rtf_file.write(\"}\")\n",
    "\n",
    "print(f\"Terminal output saved to {rtf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
